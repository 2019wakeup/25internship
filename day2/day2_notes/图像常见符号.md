**神经网络中的逐元素操作**

神经网络中两种基础且重要的逐元素（Element-wise）操作，常用于特征融合、信息调节与门控。**关键前提**：参与运算的张量（Tensor）通常需要具有**完全相同的形状**（Shape），或者满足广播（Broadcasting）机制的要求。

**1. 逐元素加法 (Element-wise Addition)**

*   **符号:** `⊕` 或 `+`
*   **核心操作:** 将两个（或多个）输入张量中**对应位置**的元素**直接相加**。
    *   `Output[i, j, k] = Input_A[i, j, k] + Input_B[i, j, k]`
*   **输出形状:** 与输入张量形状**相同**。
*   **主要应用:**
    *   **残差连接 (Residual Connections / Skip Connections):** 如 ResNet 结构。将输入特征 `x` （通过捷径）与经过若干层处理后的特征 `F(x)` 相加，形成 `H(x) = F(x) + x`。
*   **核心目的 (在 ResNet 中):**
    *   **组合信息：** 将原始信息与处理后的增量（残差）信息结合。
    *   **改善梯度流：** 提供梯度传播的“高速公路”，缓解深度网络中的梯度消失问题，使得训练更深的网络成为可能。

**2. 逐元素乘法 (Element-wise Multiplication / Hadamard Product)**

*   **符号:** `⊗` 或 `*` (在代码中), `⊙` (有时在数学表示中)
*   **核心操作:** 将两个（或多个）输入张量中**对应位置**的元素**直接相乘**。
    *   `Output[i, j, k] = Input_A[i, j, k] * Input_B[i, j, k]`
*   **输出形状:** 与输入张量形状**相同**（或遵循广播规则）。
*   **主要应用:**
    *   **注意力机制 (Attention Mechanisms):** 将计算出的注意力权重图（或向量）与原始特征图相乘，以**增强重要特征**、**抑制次要特征**。
    *   **门控机制 (Gating Mechanisms):** 如 LSTM、GRU 中的遗忘门、输入门、输出门等。用一个介于 0~1 之间的门控值向量（或张量）与信息流相乘，以**控制信息通过的程度**（允许、阻止或部分允许）。
    *   **特征调制 (Feature Modulation):** 用一个张量去动态地**缩放或调整**另一个张量的特征。
*   **核心目的:**
    *   **调节/缩放：** 根据一个张量的值来调整另一个张量的值。
    *   **门控/过滤：** 选择性地让信息通过或阻止信息。

**关键区别总结:**

*   **加法 (⊕):** 通常用于**合并**或**组合**信息流（如原始信息 + 变化量），尤其有利于梯度传播。
*   **乘法 (⊗):** 通常用于**调节**、**缩放**或**门控**信息流，实现基于重要性或条件的特征选择/过滤。
