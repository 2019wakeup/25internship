这两个操作的核心目的都是改变数据（通常是图像或特征图）的**空间分辨率 (Spatial Resolution)**，即宽度和高度。

---

**一、 下采样 (Downsampling / Subsampling)**

*   **定义:** 下采样是指**降低**图像或特征图的空间分辨率（即减少像素数量，使图像变小）。
*   **目的/作用:**
    1.  **减少计算量和内存占用:** 特征图尺寸变小，后续层的计算负担减轻。
    2.  **增大感受野 (Receptive Field):** 如我们之前讨论的，通过下采样（尤其是池化），后续层的神经元能够“看到”原始输入图像中更大的区域。这有助于网络捕捉更大范围的上下文信息和更抽象的特征。
    3.  **提升特征的不变性 (Invariance):** 尤其是最大池化，可以对微小的平移、旋转或变形提供一定程度的不变性，因为只保留了局部区域最显著的特征。
    4.  **构建特征金字塔:** 在经典的 CNN 结构中，通过逐层下采样，网络能够学习到从低级细节到高级语义的层级化特征表示。

*   **常见方法:**
    1.  **池化 (Pooling):** 这是 CNN 中最常用的下采样方法。
        *   **最大池化 (Max Pooling):** 在一个窗口（如 2x2）内，选取最大的值作为输出。倾向于保留最显著的特征（纹理、边缘等）。
        *   **平均池化 (Average Pooling):** 在一个窗口内，计算所有值的平均值作为输出。倾向于保留更平滑、包含背景信息的特征。
        *   **步幅 (Stride):** 池化操作通常伴随着大于 1 的步幅（如 stride=2），这直接导致了输出尺寸的减小。一个 2x2 的池化窗口，步幅为 2，会将输入尺寸减半。
    2.  **步长卷积 (Strided Convolution):** 使用卷积核进行卷积操作时，设置步幅 (Stride) 大于 1（通常为 2）。
        *   **优势:** 这种方式将下采样和特征提取结合在一步完成，并且是**可学习的**（卷积核的权重可以训练），相比固定的池化操作可能更灵活，信息损失也可能更少。近年来在很多网络结构中逐渐替代池化层。
    3.  **插值法 (Interpolation - 用于下采样):** 虽然不常用在 CNN 的特征提取阶段，但理论上也可以用插值方法（如最近邻、双线性插值）来缩小图像，但这通常会丢失较多信息，且不是 CNN 的标准做法。

---

**二、 上采样 (Upsampling)**

*   **定义:** 上采样是指**提高**图像或特征图的空间分辨率（即增加像素数量，使图像变大）。
*   **目的/作用:**
    1.  **恢复空间分辨率:** 在需要进行像素级预测的任务中（如语义分割、实例分割、关键点检测、深度估计等），需要将深度网络提取的低分辨率、高语义的特征图恢复到较高的空间分辨率，以便进行精确定位。
    2.  **特征融合:** 在像 U-Net 或特征金字塔网络 (FPN) 这样的架构中，需要将低分辨率的深层特征图上采样，使其尺寸与高分辨率的浅层特征图匹配，然后进行融合（如相加或拼接），结合两者的优势（深层语义信息 + 浅层细节信息）。
    3.  **图像生成/超分辨率:** 在生成对抗网络 (GAN) 或图像超分辨率任务中，需要从低维度的潜在向量或低分辨率图像生成高分辨率的图像。

*   **常见方法:**
    1.  **插值法 (Interpolation):** 这是最简单的上采样方法，通过数学公式计算新像素的值。
        *   **最近邻插值 (Nearest Neighbor Interpolation):** 将新位置的像素值设为最近的原始像素值。速度快，但容易产生块状效应 (blockiness)。
        *   **双线性插值 (Bilinear Interpolation):** 新像素的值是其周围 4 个原始像素值的加权平均。效果比最近邻平滑，是常用的默认方法。
        *   **双三次插值 (Bicubic Interpolation):** 考虑周围 16 个像素进行加权平均，效果通常更平滑、更清晰，但计算量更大。
    2.  **转置卷积 (Transposed Convolution / Deconvolution):** 这是 CNN 中最常用的**可学习的**上采样方法。
        *   **原理:** 可以理解为卷积操作的“逆过程”（但并非严格的逆）。它将输入特征图上的每个单元“投射”到一个更大的输出区域，并使用可学习的卷积核来控制如何填充这个区域。它本质上还是一种卷积，只是其输入输出的空间关系与普通卷积相反。
        *   **注意:** 有时被称为 "Deconvolution"，但这在数学上并不准确，"Transposed Convolution" 是更精确的术语。
        *   **问题:** 容易产生棋盘状伪影 (Checkerboard Artifacts)，需要仔细设计卷积核大小和步幅，或配合后续卷积层来缓解。
    3.  **像素重组 / 深度到空间 (PixelShuffle / DepthToSpace):**
        *   **原理:** 先通过卷积生成一个通道数很多 (C * r^2) 但空间分辨率不变的特征图，然后将这些通道上的值重新排列（Shuffle）到一个空间分辨率扩大 r 倍、通道数减少到 C 的新特征图上。
        *   **优势:** 通常能产生比转置卷积更平滑、伪影更少的上采样结果，在超分辨率等任务中很受欢迎。
    4.  **上采样 + 卷积 (Upsample + Convolution):**
        *   **原理:** 先使用简单的插值方法（如双线性插值）将特征图放大，然后再接一个普通的卷积层（通常是 3x3 卷积）。
        *   **优势:** 概念简单，实现方便，可以有效避免转置卷积的棋盘格问题。卷积层可以学习如何更好地细化插值后的特征。这种方法在现代网络设计中越来越常见。

---

**总结:**

*   **下采样**是为了**减少维度、增大感受野、提取更抽象特征**，常用方法是**池化**和**步长卷积**。
*   **上采样**是为了**恢复维度、进行密集预测、融合多尺度特征**，常用方法是**插值**、**转置卷积**、**PixelShuffle** 以及 **插值+卷积**。

在很多现代 CNN 架构中（如 U-Net, FPN, SegNet 等），下采样（编码器部分）和上采样（解码器部分）经常成对出现，构成**编码器-解码器 (Encoder-Decoder)** 结构，以同时利用深度网络的强大语义理解能力和对空间细节的精确捕捉能力。