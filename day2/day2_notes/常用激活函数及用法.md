**什么是激活函数 (Activation Function)？**

在神经网络中，激活函数是应用于神经元（或节点）输出的函数，它决定了该神经元是否应该被“激活”（即传递信号）以及传递信号的强度。其**核心作用是引入非线性**，使得神经网络能够学习和拟合复杂的非线性模式。如果没有非线性激活函数，无论神经网络有多少层，其最终输出都将是输入的一个线性组合，这极大地限制了模型的表达能力。

**常见的激活函数及其特性与适用任务：**

1.  **Sigmoid 函数**
    *   **公式：** `σ(x) = 1 / (1 + exp(-x))`
    *   **输出范围：** (0, 1)
    *   **特点：**
        *   将输入压缩到0和1之间，可以解释为概率。
        *   平滑，易于求导。
        *   **缺点：**
            *   **梯度消失 (Vanishing Gradient):** 当输入值非常大或非常小时，函数的导数（梯度）接近于0，这会导致在反向传播过程中，梯度信号逐层递减，使得深层网络的权重更新缓慢或停止，难以训练。
            *   **输出非零中心 (Not Zero-Centered):** 输出恒大于0，这会导致后续层接收到的输入总是正的，可能影响梯度下降的效率（产生“ZigZag”现象）。
            *   计算复杂度相对较高（涉及指数运算）。
    *   **适用任务：**
        *   **二元分类 (Binary Classification) 的输出层：** 输出值可以解释为属于正类的概率。
        *   **多标签分类 (Multi-Label Classification) 的输出层：** 对每个标签独立使用Sigmoid，输出该标签存在的概率。
        *   **早期神经网络或特定门控机制 (如LSTM/GRU的某些门)：** 但在隐藏层中已基本被ReLU及其变种取代。

2.  **Tanh (双曲正切) 函数**
    *   **公式：** `tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))` 或 `2 * σ(2x) - 1`
    *   **输出范围：** (-1, 1)
    *   **特点：**
        *   将输入压缩到-1和1之间。
        *   **输出是零中心的 (Zero-Centered)：** 相比Sigmoid，这通常有助于更快地收敛。
        *   平滑，易于求导。
        *   **缺点：**
            *   仍然存在**梯度消失**的问题（虽然比Sigmoid略好，但在饱和区域梯度仍接近0）。
            *   计算复杂度相对较高。
    *   **适用任务：**
        *   **循环神经网络 (RNN) 如LSTM、GRU的隐藏层和门控单元：** 零中心特性有优势。
        *   **隐藏层：** 曾经是Sigmoid的常用替代品，但在很多场景下已被ReLU系列超越。
        *   需要输出在(-1, 1)范围内的任务。

3.  **ReLU (Rectified Linear Unit) 函数**
    *   **公式：** `ReLU(x) = max(0, x)`
    *   **输出范围：** [0, +∞)
    *   **特点：**
        *   **计算极其高效：** 只涉及简单的比较和赋值操作。
        *   **有效缓解梯度消失问题（对于正输入）：** 当输入大于0时，梯度恒为1，使得梯度可以顺畅地传播。
        *   **引入稀疏性：** 当输入小于等于0时，输出为0，使得部分神经元不被激活，网络更稀疏，可能提高泛化能力。
        *   **缺点：**
            *   **Dying ReLU Problem (神经元死亡)：** 如果一个神经元的输入在训练过程中持续为负，其梯度将始终为0，导致该神经元的权重无法更新，该神经元可能永久失效。
            *   **输出非零中心。**
    *   **适用任务：**
        *   **卷积神经网络 (CNN) 和多层感知机 (MLP) 的隐藏层：** 目前是**最常用、最主流**的默认选择，通常能带来良好的性能和训练速度。

4.  **Leaky ReLU 函数**
    *   **公式：** `LeakyReLU(x) = max(αx, x)`，其中 `α` 是一个小的正常数（如0.01）。
    *   **输出范围：** (-∞, +∞)
    *   **特点：**
        *   试图解决 "Dying ReLU" 问题：当输入为负时，允许一个小的、非零的梯度 (`α`)，使得神经元有机会恢复。
        *   保留了ReLU的大部分优点（计算效率高、正区梯度不消失）。
    *   **适用任务：**
        *   **隐藏层：** 作为ReLU的替代品，尤其是在怀疑存在大量 "Dying ReLU" 单元时。常用于CNN、GAN等。

5.  **PReLU (Parametric ReLU) 函数**
    *   **公式：** `PReLU(x) = max(αx, x)`，其中 `α` 是一个**可学习的参数**。
    *   **输出范围：** (-∞, +∞)
    *   **特点：**
        *   Leaky ReLU的泛化，负区斜率 `α` 由数据驱动学习得到，理论上更灵活。
        *   相比Leaky ReLU，增加了一些参数量和过拟合风险。
    *   **适用任务：**
        *   **隐藏层：** 效果可能优于Leaky ReLU，但需要更多数据和调整。

6.  **ELU (Exponential Linear Unit) 函数**
    *   **公式：**
        `ELU(x) = x` if `x > 0`
        `ELU(x) = α(exp(x) - 1)` if `x <= 0`，其中 `α` 通常为1。
    *   **输出范围：** (`-α`, +∞)
    *   **特点：**
        *   结合了ReLU（正区线性）和类Tanh（负区饱和）的特性。
        *   负值输出使得整体输出均值更接近0（类似Tanh）。
        *   负区的平滑性可能使模型对噪声更鲁棒。
        *   **缺点：** 计算复杂度高于ReLU和Leaky ReLU（涉及指数运算）。
    *   **适用任务：**
        *   **隐藏层：** 作为ReLU的替代品，有时能获得更好的性能，尤其是在需要更鲁棒的模型或输出接近零均值有利的情况下。

7.  **Softmax 函数**
    *   **公式：** `Softmax(x)_i = exp(x_i) / Σ_j exp(x_j)`
    *   **输出范围：** 每个输出值在 (0, 1) 之间，且所有输出值之和为 1。
    *   **特点：**
        *   将一个实数向量转换为概率分布。
        *   常用于表示互斥类别的概率。
        *   对输入的变化非常敏感。
    *   **适用任务：**
        *   **多类别分类 (Multi-Class Classification) 的输出层：** 输出向量的每个元素代表对应类别的预测概率。**这是Softmax最核心和最常见的应用场景。** 它通常**不**用于隐藏层。

**总结与选择建议：**

*   **隐藏层首选：** **ReLU** 是最常用且通常效果不错的起点。
*   **ReLU的替代：** 如果遇到 "Dying ReLU" 问题或希望探索性能提升，可以尝试 **Leaky ReLU、PReLU 或 ELU**。ELU理论上更优，但计算成本稍高。
*   **RNN相关：** **Tanh** 和 **Sigmoid** 在LSTM、GRU等门控单元中仍有应用，Tanh因零中心特性有时优于Sigmoid。
*   **输出层：**
    *   **二元分类：** **Sigmoid**
    *   **多类别分类（单选）：** **Softmax**
    *   **多标签分类（多选）：** 对每个标签使用 **Sigmoid**
    *   **回归任务：** 通常**不使用激活函数**（线性输出），或者根据输出范围要求使用特定函数（如ReLU保证非负，Tanh/Sigmoid保证范围）。

**重要提示：** 激活函数的选择并没有绝对的“最佳”，它往往依赖于具体的任务、数据分布、网络架构和超参数设置。在实践中，通常需要根据经验选择一个合适的起点（如ReLU），并通过实验来比较不同激活函数的性能，找到最适合当前问题的选择。